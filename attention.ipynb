{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport random\nfrom datetime import datetime\nimport wandb\nimport pandas\nimport requests, zipfile, io","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"75723b6e8716d094c31d6b7e25cc865ac5907e1f\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#downloading dataset by drive link\n#Ref - https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\nurl = \"https://drive.google.com/u/0/uc?id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw&export=download\"\nresponse = requests.get(url)\nz = zipfile.ZipFile(io.BytesIO(response.content))\nz.extractall()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading hindi dataset\ncsvFile = pandas.read_csv('/kaggle/working/aksharantar_sampled/hin/hin_train.csv', names = ['English', 'Hindi'])\ntrain_input = csvFile['English']\ntrain_output = csvFile['Hindi']\ncsvFile = pandas.read_csv('/kaggle/working/aksharantar_sampled/hin/hin_valid.csv', names = ['English', 'Hindi'])\nvalid_input = csvFile['English']\nvalid_output = csvFile['Hindi']\ncsvFile = pandas.read_csv('/kaggle/working/aksharantar_sampled/hin/hin_test.csv', names = ['English', 'Hindi'])\ntest_input = csvFile['English']\ntest_output = csvFile['Hindi']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SOW -> startOfWord\n#EOW -> endOfWToken\nSOW_token = 0\nEOW_token = 1\n\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.char2index = {}\n        self.index2char = {0: \"#\", 1: \"$\"}\n        self.n_chars = 2  #  2 because SOW and EOW we are adding at the start\n\n    #Takes a list of words and all words in the language (dictionary)\n    def addAllWords(self, words):\n        for word in words:\n            self.addWord(word)\n\n    #Takes a single word and then adds that word to dict\n    def addWord(self, word):\n        for c in word:\n            if c not in self.char2index:\n                self.char2index[c] = self.n_chars\n                self.index2char[self.n_chars] = c\n                self.n_chars += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create an object of Lang class and then adds all the english words of training dataset to english lang\nlang_input = Lang(\"English\")\nlang_input.addAllWords(train_input)\n#Create an object of Lang class and then adds all the hindi words of training dataset to hindi lang\nlang_output = Lang(\"Hindi\")\nlang_output.addAllWords(train_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout_p, batch_size, embedding_size, cell_type = \"LSTM\", bidirection = False):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.dropout = nn.Dropout(dropout_p)\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.bidirection = bidirection\n        if(cell_type == \"GRU\"):\n            self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bidirection)\n        elif(cell_type == \"LSTM\"):\n            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bidirection)\n        elif(cell_type == \"RNN\"):\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bidirection)\n\n    def forward(self, input, hidden):\n        #In case of LSTM, hidden is a tuple which contains (hidden, cell)\n        embedded = self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n        output = self.dropout(embedded)\n        if(self.cell_type == \"GRU\"):\n            output, hidden = self.gru(output, hidden)\n        elif(self.cell_type == \"LSTM\"):\n            output, (hidden, cell) = self.lstm(output)\n        elif(self.cell_type == \"RNN\"):\n            output, hidden = self.rnn(output, hidden)\n        #In case of bidirection, the size of hidden is (2*num_layers, batch_size, hidden_size)\n        #But for decoder, we need hidden of size (num_layers, batch_size, hidden_size)\n        #Therefore we are taking avg of hidden[:num_layers] and hidden[num_layers:]\n        #Similarly for cell and output\n        if self.bidirection:\n            hidden = hidden.reshape(2, hidden.size(0)//2, hidden.size(1), hidden.size(2))\n            hidden = torch.add(hidden[0]*0.5, hidden[1]*0.5)\n            if(self.cell_type == \"LSTM\"):\n                cell = cell.reshape(2, cell.size(0)//2, cell.size(1), cell.size(2))\n                cell = torch.add(cell[0]*0.5, cell[1]*0.5)\n            output = output.permute(2, 1, 0)\n            output = torch.split(output, output.shape[0]//2)\n            output = torch.add(output[0].permute(2, 1, 0)*0.5, output[1].permute(2, 1, 0)*0.5)\n        \n        if self.cell_type == \"LSTM\":\n            return output, hidden, cell\n        else:\n            return output, hidden\n\n    def initHidden(self):\n        if self.bidirection:\n            return torch.zeros(2*self.num_layers, self.batch_size, self.hidden_size, device=device)\n        else:\n            return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate maxlength of word in train, valid and test dataset\nmax_length = 0\nfor i in train_input:\n    max_length = max(max_length, len(i))\nfor i in valid_input:\n    max_length = max(max_length, len(i))\nfor i in test_input:\n    max_length = max(max_length, len(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, num_layers, dropout_p, batch_size, embedding_size, cell_type = \"LSTM\"):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.embedding = nn.Embedding(output_size, self.embedding_size)\n        self.cell_type = cell_type\n        #(hidden size +embedding) * max_length\n        self.attn = nn.Linear(self.hidden_size + self.embedding_size, max_length+1)\n        # hidden + embedding , hidden\n        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        if(self.cell_type == \"GRU\"):\n            self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.num_layers, dropout = self.dropout_p)\n        elif(self.cell_type == \"LSTM\"):\n            self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers, dropout = self.dropout_p)\n        elif(self.cell_type == \"RNN\"):\n            self.rnn = nn.RNN(self.hidden_size, self.hidden_size, self.num_layers, dropout = dropout_p)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden, encoder_outputs):\n        # input -> 1*batch_size\n        input = input.unsqueeze(0)\n        output = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n        output = self.dropout(output)\n        #batch_size * seq_length\n        if self.cell_type == \"LSTM\":\n            attn_weights = F.softmax(self.attn(torch.cat((output[0], hidden[0][0]), 1)), dim=1)\n        else:\n            attn_weights = F.softmax(self.attn(torch.cat((output[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n                                 encoder_outputs.permute(1, 0, 2))\n        attn_applied = attn_applied.squeeze(1)\n        output = torch.cat((output[0], attn_applied), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n        output = F.relu(output)\n        if(self.cell_type == \"GRU\"):\n            output, hidden = self.gru(output, hidden)\n        elif(self.cell_type == \"LSTM\"):\n            output, (hidden, cell) = self.lstm(output, (hidden[0], hidden[1]))\n        elif(self.cell_type == \"RNN\"):\n            output, hidden = self.rnn(output, hidden)\n        if self.cell_type == \"LSTM\":\n            return self.out(output[0]), hidden, cell, attn_weights\n        return self.out(output[0]), hidden, attn_weights\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Maps each char of word from char to index in that lang\n#Output is a list\ndef indexesFromWord(lang, word):\n    return [lang.char2index[c] for c in word]\n\n#First calls indexesFromWord to get the mapping of each char to its index in that lang\n#And then converts that list of indexes to tensor\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, cell_type):\n    teacher_forcing_ratio = 0.5\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n    loss = 0\n    if cell_type == \"LSTM\":\n        encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor, encoder_hidden)\n    else:\n        encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n    decoder_input = torch.tensor([SOW_token]*batch_size, device=device)\n    decoder_hidden = encoder_hidden\n    \n    if cell_type == \"LSTM\":\n        decoder_cell = encoder_cell\n    \n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        #Approaximately 50% of the time if will be executed\n        #In this case, for the prediction of current char, input will be last actual char instead of last predicted char\n        for di in range(target_length):\n            if cell_type == \"LSTM\":\n                decoder_output, decoder_hidden, decoder_cell, attn_weights = decoder(decoder_input, (decoder_hidden, decoder_cell), encoder_output)\n            else:\n                decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n            loss += criterion(decoder_output, target_tensor[di])\n            #Input to next cell, will be current hindi char, not curr predicted char\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            if cell_type == \"LSTM\":\n                decoder_output, decoder_hidden, decoder_cell, attn_weights = decoder(decoder_input, (decoder_hidden, decoder_cell), encoder_output)\n            else:\n                decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  \n            loss += criterion(decoder_output, target_tensor[di])\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length,  attn_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Append all the words till max_length\ndef getBatchedTensorFromWords(words, batch_size, lang):\n    input_tensor = [tensorFromWord(lang, word) for word in words]\n    batched_tensors = ((nn.utils.rnn.pad_sequence(input_tensor).squeeze(2)).to(device))\n    batchWise = []\n    for i in range(0, batched_tensors.shape[1], batch_size):\n        currBatch = (batched_tensors[0:batched_tensors.shape[0], i:(i+batch_size)])\n        if currBatch.shape[0] < max_length+1:\n            padding_tensor = torch.zeros(max_length+1-currBatch.shape[0], currBatch.shape[1], dtype = currBatch.dtype, device=device)\n            currBatch = torch.cat((currBatch, padding_tensor), dim = 0)\n        batchWise.append(currBatch)\n    \n    return batchWise","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainIters(encoder, decoder, n_datapoints, epochs, learning_rate, batch_size, embedding_size, cell_type, num_layers_encoder, num_layers_decoder, hidden_size, bidirectional, dropout_encoder, dropout_decoder):\n    run_name = \"embS_{}_nlEnc_{}_nlDec_{}_hl_{}_cellType_{}_biDir_{}_dropEnc_{}_dropDec_{}_ep_{}_bs_{}\".format(embedding_size, num_layers_encoder, num_layers_decoder, hidden_size, cell_type, bidirectional, dropout_encoder, dropout_decoder, epochs, batch_size)\n    print_loss_total = 0  \n\n    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=learning_rate, weight_decay = 0.0005)\n    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=learning_rate, weight_decay = 0.0005)\n    criterion = nn.CrossEntropyLoss()\n    \n    #Convert data to batched data\n    train_batch_input = getBatchedTensorFromWords(train_input, batch_size, lang_input)\n    train_batch_target = getBatchedTensorFromWords(train_output, batch_size, lang_output)\n    \n    valid_batch_input = getBatchedTensorFromWords(valid_input, batch_size, lang_input)\n    \n    for epochNum in range(epochs):\n        for i in range(len(train_batch_input)):\n            #Call the train function for one batch\n            loss, attn = train(train_batch_input[i], train_batch_target[i], encoder,\n                         decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, cell_type)\n            print_loss_total += loss*batch_size\n\n        print_loss_avg = print_loss_total / len(train_input)\n        print_loss_total = 0\n        print(\"Average loss after \", epochNum+1, \"epochs is \", print_loss_avg)\n        valid_accuracy = findAccuracy(encoder, decoder, valid_batch_input, valid_output, cell_type, len(valid_input), batch_size, False)\n        print(\"Valid accuracy is \", valid_accuracy)\n        wandb.log({\"validation_accuracy\": valid_accuracy, \"training_loss\": print_loss_avg, 'epoch': epochNum})\n    \n    train_accuracy = findAccuracy(encoder, decoder, train_batch_input, train_output, cell_type, len(train_input), batch_size, False)\n    print(\"Train accuracy is \", train_accuracy)\n    \n    valid_accuracy = findAccuracy(encoder, decoder, valid_batch_input, valid_output, cell_type, len(valid_input), batch_size, False)\n    print(\"Valid accuracy is \", valid_accuracy)\n    \n    test_batch_input = getBatchedTensorFromWords(test_input, batch_size, lang_input)\n    test_accuracy = findAccuracy(encoder, decoder, test_batch_input, test_output, cell_type, len(test_input), batch_size, True)\n    \n    \n    wandb.log({\"training_accuracy\": train_accuracy})\n    wandb.run.name = run_name\n    wandb.run.save()\n    wandb.run.finish()\n    return attn\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Takes the model and one batch as input\n#outputs predicted word for each english of the batch\ndef evaluate(encoder, decoder, input_tensors, cell_type, batch_size):\n    with torch.no_grad():\n        \n        input_length = input_tensors.size(0)\n        encoder_hidden = encoder.initHidden()\n\n        if cell_type == \"LSTM\":\n            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensors, encoder_hidden)\n        else:\n            encoder_output, encoder_hidden = encoder(input_tensors, encoder_hidden)\n\n        decoder_input = torch.tensor([SOW_token]*batch_size, device=device)  # SOW\n\n        decoder_hidden = encoder_hidden\n\n        if cell_type == \"LSTM\":\n            decoder_cell = encoder_cell\n\n        decoded_words = [\"\"]*batch_size\n\n        for di in range(input_length):\n            \n            if cell_type == \"LSTM\":\n                decoder_output, decoder_hidden, decoder_cell, attn_weights = decoder(decoder_input, (decoder_hidden, decoder_cell), encoder_output)\n            else:\n                decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n            topv, topi = decoder_output.data.topk(1)\n            #If curr predicted is eow or padded char, then ignore\n            #else add curr predicted char to decoded_words list\n            for i in range(batch_size):\n                if topi[i].item() == EOW_token or topi[i] == 0:\n                    continue\n                else:\n                    decoded_words[i] += lang_output.index2char[topi[i].item()]\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def findAccuracy(encoder1, decoder1, input, actual_output, cell_type, n, batch_size, flag):\n    correct = 0\n    wrong = 0\n    file1 = open(\"testDatasetWordsWithAttention.txt\",\"a\")\n    file1.write(\"Correct word      Predicted word\\n\")\n    for i in range(len(input)):\n        output_word = evaluate(encoder1, decoder1, input[i], cell_type, batch_size)\n        for j in range(i*batch_size, i*batch_size+batch_size):\n            if(actual_output[j] == output_word[j-i*batch_size]):\n                correct += 1\n            elif flag:\n                wrong += 1\n                s = str(wrong) + actual_output[j] + \"   \" + output_word[j-i*batch_size] + \"\\n\"\n                file1.write(s)\n    file1.close()\n    return 100*correct/n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_defaults = {\n    \"embedding_size\": 64,\n    \"num_layers_encoder\": 3,\n    \"num_layers_decoder\": 3,\n    \"hidden_layer\": 256,\n    \"cell_type\": \"LSTM\",\n    \"bidirectional\": True,\n    \"dropout_encoder\": 0.2,\n    \"dropout_decoder\": 0.3,\n    \"epochs\": 20,\n    \"batch_size\": 128,\n}\nwandb.init(project = 'Assignment 3', entity = 'cs22m006', config=config_defaults)\nembedding_size = wandb.config.embedding_size\nnum_layers_encoder = wandb.config.num_layers\nnum_layers_decoder = wandb.config.num_layers\nhidden_size = wandb.config.hidden_layer\nbatch_size = wandb.config.batch_size\nepochs = wandb.config.epochs\ncell_type = wandb.config.cell_type\nbidirectional = wandb.config.bidirectional\ndropout_encoder = wandb.config.dropout_encoder\ndropout_decoder = wandb.config.dropout_decoder\nlearning_rate = 0.001\n\n#Comment above lines of this cell and uncomment below lines of this cell, if you want to just test the model without wandb\n# embedding_size = 64\n# num_layers_encoder = 1\n# num_layers_decoder = 1\n# hidden_size = 512\n# batch_size = 64\n# epochs = 30\n# cell_type = \"LSTM\"\n# bidirectional = True\n# dropout_encoder = 0.4\n# dropout_decoder = 0.4\n# learning_rate = 0.001\n# encoder1 = EncoderRNN(lang_input.n_chars, hidden_size, num_layers_encoder, dropout_encoder, batch_size, embedding_size, cell_type, bidirectional).to(device)\n# decoder1 = AttnDecoderRNN(hidden_size, lang_output.n_chars, num_layers_decoder, dropout_decoder, batch_size, embedding_size, cell_type).to(device)\nattn = trainIters(encoder1, decoder1, len(train_input), epochs, learning_rate, batch_size, embedding_size, cell_type, num_layers_encoder, num_layers_decoder, hidden_size, bidirectional, dropout_encoder, dropout_decoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run this cell, if you want to run sweep\nsweep_config = {\n  \"name\": \"CS6910 Assignment 3 - Cross Entropy Loss\",\n  \"metric\": {\n      \"name\":\"validation_accuracy\",\n      \"goal\": \"maximize\"\n  },\n  \"method\": \"bayes\",\n  \"parameters\": {\n        \"embedding_size\": {\n            \"values\": [512, 256, 64, 32]\n        },\n        \"num_layers\": {\n            \"values\": [3, 2, 1]\n        },\n        \"hidden_layer\": {\n            \"values\": [512, 256, 128]\n        },\n        \"cell_type\": {\n            \"values\": [\"RNN\", \"GRU\", \"LSTM\"]\n        },\n        \"bidirectional\": {\n            \"values\": [False, True]\n        },\n        \"dropout_encoder\": {\n            \"values\": [0.2, 0.3, 0.4]\n        },\n        \"dropout_decoder\": {\n            \"values\": [0.2, 0.3, 0.4]\n        },\n        \"epochs\": {\n            \"values\": [20, 30]\n        },\n        \"batch_size\": {\n            \"values\": [256, 128, 64, 32]\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep_config, entity=\"cs22m006\", project=\"Assignment 3\")\nwandb.agent(sweep_id, runSweep, count = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}