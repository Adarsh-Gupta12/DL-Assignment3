{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport random\nfrom datetime import datetime\nimport wandb\nimport requests, zipfile, io\nimport pandas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#downloading dataset by drive link\n#Ref - https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\nurl = \"https://drive.google.com/u/0/uc?id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw&export=download\"\nresponse = requests.get(url)\nz = zipfile.ZipFile(io.BytesIO(response.content))\nz.extractall()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"75723b6e8716d094c31d6b7e25cc865ac5907e1f\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading hindi dataset\ncsvFile = pandas.read_csv('/kaggle/working/aksharantar_sampled/hin/hin_train.csv', names = ['English', 'Hindi'])\ntrain_input = csvFile['English']\ntrain_output = csvFile['Hindi']\ncsvFile = pandas.read_csv('/kaggle/working/aksharantar_sampled/hin/hin_valid.csv', names = ['English', 'Hindi'])\nvalid_input = csvFile['English']\nvalid_output = csvFile['Hindi']\ncsvFile = pandas.read_csv('/kaggle/working/aksharantar_sampled/hin/hin_test.csv', names = ['English', 'Hindi'])\ntest_input = csvFile['English']\ntest_output = csvFile['Hindi']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SOW -> startOfWord\n#EOW -> endOfWToken\nSOW_token = 0\nEOW_token = 1\n\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.char2index = {}\n        self.index2char = {0: \"#\", 1: \"$\"}\n        self.n_chars = 2  #  2 because SOW and EOW we are adding at the start\n\n    #Takes a list of words and all words in the language (dictionary)\n    def addAllWords(self, words):\n        for word in words:\n            self.addWord(word)\n\n    #Takes a single word and then adds that word to dict\n    def addWord(self, word):\n        for c in word:\n            if c not in self.char2index:\n                self.char2index[c] = self.n_chars\n                self.index2char[self.n_chars] = c\n                self.n_chars += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create an object of Lang class and then adds all the english words of training dataset to english lang\nlang_input = Lang(\"English\")\nlang_input.addAllWords(train_input)\n#Create an object of Lang class and then adds all the hindi words of training dataset to hindi lang\nlang_output = Lang(\"Hindi\")\nlang_output.addAllWords(train_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout_p, batch_size, embedding_size, cell_type = \"LSTM\", bidirection = False):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.dropout = nn.Dropout(dropout_p)\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.bidirection = bidirection\n        if(cell_type == \"GRU\"):\n            self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bidirection)\n        elif(cell_type == \"LSTM\"):\n            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bidirection)\n        elif(cell_type == \"RNN\"):\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bidirection)\n\n    def forward(self, input, hidden):\n        #In case of LSTM, hidden is a tuple which contains (hidden, cell)\n        embedded = self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n        output = self.dropout(embedded)\n        if(self.cell_type == \"GRU\"):\n            _, hidden = self.gru(output, hidden)\n        elif(self.cell_type == \"LSTM\"):\n            _, (hidden, cell) = self.lstm(output)\n        elif(self.cell_type == \"RNN\"):\n            _, hidden = self.rnn(output, hidden)\n        if self.bidirection:\n            #In case of bidirection, the size of hidden is (2*num_layers, batch_size, hidden_size)\n            #But for decoder, we need hidden of size (num_layers, batch_size, hidden_size)\n            #Therefore we are taking avg of hidden[:num_layers] and hidden[num_layers:]\n            hidden = hidden.reshape(2, hidden.size(0)//2, hidden.size(1), hidden.size(2))\n            hidden = torch.add(hidden[0]*0.5, hidden[1]*0.5)\n            hidden = hidden.squeeze(0)\n            if(self.cell_type == \"LSTM\"):\n                cell = cell.reshape(2, cell.size(0)//2, cell.size(1), cell.size(2))\n                cell = torch.add(cell[0]*0.5, cell[1]*0.5)\n                cell = cell.squeeze(0)\n        if self.cell_type == \"LSTM\":\n            return hidden, cell\n        else:\n            return hidden\n\n    def initHidden(self):\n        if self.bidirection:\n            return torch.zeros(2*self.num_layers, self.batch_size, self.hidden_size, device=device)\n        else:\n            return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, num_layers, dropout_p, batch_size, embedding_size, cell_type = \"LSTM\"):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.dropout = nn.Dropout(dropout_p)\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.embedding = nn.Embedding(output_size, embedding_size)\n        self.cell_type = cell_type\n        if(cell_type == \"GRU\"):\n            self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout = dropout_p)\n        elif(cell_type == \"LSTM\"):\n            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = dropout_p)\n        elif(cell_type == \"RNN\"):\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout = dropout_p)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        #In case of LSTM, hidden is a tuple which contains (hidden, cell)\n        output = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n        output = self.dropout(output)\n        output = torch.relu(output)\n        if(self.cell_type == \"GRU\"):\n            output, hidden = self.gru(output, hidden)\n        elif(self.cell_type == \"LSTM\"):\n            output, (hidden, cell) = self.lstm(output, (hidden[0], hidden[1]))\n        elif(self.cell_type == \"RNN\"):\n            output, hidden = self.rnn(output, hidden)\n        if self.cell_type == \"LSTM\":\n            return self.softmax(self.out(output[0])), hidden, cell\n        return self.softmax(self.out(output[0])), hidden\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Maps each char of word from char to index in that lang\n#Output is a list\ndef indexesFromWord(lang, word):\n    return [lang.char2index[c] for c in word]\n\n#First calls indexesFromWord to get the mapping of each char to its index in that lang\n#And then converts that list of indexes to tensor\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    #Append EOW_Token at the end of word\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model for one batch\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, cell_type):\n    teacher_forcing_ratio = 0.5\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    loss = 0\n    if cell_type == \"LSTM\":\n        encoder_hidden, encoder_cell = encoder(input_tensor, encoder_hidden)\n    else:\n        encoder_hidden = encoder(input_tensor, encoder_hidden)\n        \n    decoder_input = torch.tensor([SOW_token]*batch_size, device=device)\n    decoder_hidden = encoder_hidden\n    \n    if cell_type == \"LSTM\":\n        decoder_cell = encoder_cell\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n    \n    #Approaximately 50% of the time if will be executed\n    #In this case, for the prediction of current char, input will be last actual char instead of last predicted char\n    if use_teacher_forcing:\n        for di in range(target_length):\n            if cell_type == \"LSTM\":\n                decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n            else:\n                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            loss += criterion(decoder_output, target_tensor[di])\n            #Input to next cell, will be current hindi char, not curr predicted char\n            decoder_input = target_tensor[di] \n\n    else:\n        for di in range(target_length):\n            if cell_type == \"LSTM\":\n                decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n            else:\n                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            topv, topi = decoder_output.topk(1)\n            #Input to next cell will be current predicted hindi char\n            decoder_input = topi.squeeze().detach()  \n\n            loss += criterion(decoder_output, target_tensor[di])\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getBatchedTensorFromWords(words, batch_size, lang):\n    input_tensor = [tensorFromWord(lang, word) for word in words]\n    #Pad each word so that length of each word becomes same as max length of all words\n    #Ref -> https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n    batched_tensors = ((nn.utils.rnn.pad_sequence(input_tensor).squeeze(2)).to(device))\n    batchWise = []\n    for i in range(0, batched_tensors.shape[1], batch_size):\n        batchWise.append((batched_tensors[0:batched_tensors.shape[0], i:(i+batch_size)]))\n    return batchWise","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainIters(encoder, decoder, n_datapoints, epochs, learning_rate, batch_size, embedding_size, cell_type, num_layers_encoder, num_layers_decoder, hidden_size, bidirectional, dropout_encoder, dropout_decoder):\n    run_name = \"embS_{}_nlEnc_{}_nlDec_{}_hl_{}_cellType_{}_biDir_{}_dropEnc_{}_dropDec_{}_ep_{}_bs_{}\".format(embedding_size, num_layers_encoder, num_layers_decoder, hidden_size, cell_type, bidirectional, dropout_encoder, dropout_decoder, epochs, batch_size)\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=learning_rate, weight_decay = 0.0005)\n    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=learning_rate, weight_decay = 0.0005)\n    criterion = nn.CrossEntropyLoss()\n    \n    #Convert data to batched data\n    train_batch_input = getBatchedTensorFromWords(train_input, batch_size, lang_input)\n    train_batch_target = getBatchedTensorFromWords(train_output, batch_size, lang_output)\n    \n    valid_batch_input = getBatchedTensorFromWords(valid_input, batch_size, lang_input)\n    \n    for epochNum in range(epochs):\n        for i in range(len(train_batch_input)):\n            #Call the train function for one batch\n            loss = train(train_batch_input[i], train_batch_target[i], encoder,\n                         decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, cell_type)\n            print_loss_total += loss*batch_size\n        print_loss_avg = print_loss_total / len(train_input)\n        print_loss_total = 0\n        print(\"Average loss after \", epochNum+1, \"epochs is \", print_loss_avg)\n\n        valid_accuracy = findAccuracy(encoder, decoder, valid_batch_input, valid_output, cell_type, len(valid_input), batch_size, False)\n        print(\"Valid accuracy is \", valid_accuracy)\n        wandb.log({\"validation_accuracy\": valid_accuracy, \"training_loss\": print_loss_avg, 'epoch': epochNum})\n    \n    train_accuracy = findAccuracy(encoder, decoder, train_batch_input, train_output, cell_type, len(train_input), batch_size, False)\n    print(\"Train accuracy is \", train_accuracy)\n    wandb.log({\"training_accuracy\": train_accuracy})\n    wandb.run.name = run_name\n    wandb.run.save()\n    wandb.run.finish()\n    test_batch_input = getBatchedTensorFromWords(test_input, batch_size, lang_input)\n    test_accuracy = findAccuracy(encoder, decoder, test_batch_input, test_output, cell_type, len(test_input), batch_size, True)\n    print(\"Test accuracy\", test_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Takes the model and one batch as input\n#outputs predicted word for each english of the batch\ndef evaluate(encoder, decoder, input_tensors, cell_type, batch_size):\n    with torch.no_grad():\n        \n        input_length = input_tensors.size(0)\n        encoder_hidden = encoder.initHidden()\n\n        if cell_type == \"LSTM\":\n            encoder_hidden, encoder_cell = encoder(input_tensors, encoder_hidden)\n        else:\n            encoder_hidden = encoder(input_tensors, encoder_hidden)\n\n        decoder_input = torch.tensor([SOW_token]*batch_size, device=device)  \n        decoder_hidden = encoder_hidden\n\n        if cell_type == \"LSTM\":\n            decoder_cell = encoder_cell\n\n        decoded_words = [\"\"]*batch_size\n\n        for di in range(input_length):\n            \n            if cell_type == \"LSTM\":\n                decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n            else:\n                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            topv, topi = decoder_output.data.topk(1)\n            for i in range(batch_size):\n                #If curr predicted is eow or padded char, then ignore\n                #else add curr predicted char to decoded_words list\n                if topi[i].item() == EOW_token or topi[i] == 0:\n                    continue\n                else:\n                    decoded_words[i] += lang_output.index2char[topi[i].item()]\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def findAccuracy(encoder1, decoder1, input, actual_output, cell_type, n, batch_size, flag):\n    correct = 0\n    wrong = 0\n    if flag:\n        file1 = open(\"testDatasetWordsWithoutAttention.txt\",\"a\")\n        file1.write(\"Correct word      Predicted word\\n\")\n    for i in range(len(input)):\n        output_word = evaluate(encoder1, decoder1, input[i], cell_type, batch_size)\n        for j in range(i*batch_size, i*batch_size+batch_size):\n            if(actual_output[j] == output_word[j-i*batch_size]):\n                correct += 1\n            elif flag:\n                wrong += 1\n                s = str(wrong) + \" \" + actual_output[j] + \"   \" + output_word[j-i*batch_size] + \"\\n\"\n                file1.write(s)\n    if flag:\n        file1.close()\n    return correct/n*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runSweep():\n    config_defaults = {\n        \"embedding_size\": 64,\n        \"num_layers_encoder\": 3,\n        \"num_layers_decoder\": 3,\n        \"hidden_layer\": 256,\n        \"cell_type\": \"LSTM\",\n        \"bidirectional\": True,\n        \"dropout_encoder\": 0.2,\n        \"dropout_decoder\": 0.3,\n        \"epochs\": 20,\n        \"batch_size\": 128,\n    }\n    wandb.init(project = 'Assignment 3', entity = 'cs22m006', config=config_defaults)\n    embedding_size = wandb.config.embedding_size\n    num_layers_encoder = wandb.config.num_layers\n    num_layers_decoder = wandb.config.num_layers\n    hidden_size = wandb.config.hidden_layer\n    batch_size = wandb.config.batch_size\n    epochs = wandb.config.epochs\n    cell_type = wandb.config.cell_type\n    bidirectional = wandb.config.bidirectional\n    dropout_encoder = wandb.config.dropout_encoder\n    dropout_decoder = wandb.config.dropout_decoder\n    learning_rate = 0.001\n\n    #Comment above lines of this cell and uncomment below lines of this cell, if you want to just test the model without wandb\n#     embedding_size = 256\n#     num_layers_encoder = 3\n#     num_layers_decoder = 3\n#     hidden_size = 512\n#     batch_size = 32\n#     epochs = 30\n#     cell_type = \"LSTM\"\n#     bidirectional = True\n#     dropout_encoder = 0.4\n#     dropout_decoder = 0.2\n#     learning_rate = 0.001\n#     encoder1 = EncoderRNN(lang_input.n_chars, hidden_size, num_layers_encoder, dropout_encoder, batch_size, embedding_size, cell_type, bidirectional).to(device)\n#     decoder1 = DecoderRNN(hidden_size, lang_output.n_chars, num_layers_decoder, dropout_decoder, batch_size, embedding_size, cell_type).to(device)\n    trainIters(encoder1, decoder1, len(train_input), epochs, learning_rate, batch_size, embedding_size, cell_type, num_layers_encoder, num_layers_decoder, hidden_size, bidirectional, dropout_encoder, dropout_decoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"runSweep()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run this cell, if you want to run sweep\nsweep_config = {\n  \"name\": \"CS6910 Assignment 3 - Cross Entropy Loss\",\n  \"metric\": {\n      \"name\":\"validation_accuracy\",\n      \"goal\": \"maximize\"\n  },\n  \"method\": \"bayes\",\n  \"parameters\": {\n        \"embedding_size\": {\n            \"values\": [512, 256, 64, 32]\n        },\n        \"num_layers\": {\n            \"values\": [3, 2, 1]\n        },\n        \"hidden_layer\": {\n            \"values\": [512, 256, 128]\n        },\n        \"cell_type\": {\n            \"values\": [\"RNN\", \"GRU\"]\n        },\n        \"bidirectional\": {\n            \"values\": [False, True]\n        },\n        \"dropout_encoder\": {\n            \"values\": [0.2, 0.3, 0.4]\n        },\n        \"dropout_decoder\": {\n            \"values\": [0.2, 0.3, 0.4]\n        },\n        \"epochs\": {\n            \"values\": [20, 30]\n        },\n        \"batch_size\": {\n            \"values\": [256, 128, 64, 32]\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep_config, entity=\"cs22m006\", project=\"Assignment 3\")\nwandb.agent(sweep_id, runSweep, count = 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}